{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE_BYTE = 512 * 1024 * 1024\n",
    "\n",
    "EDGE_FILE = '/mnt/nvme2/data/papers100M/raw/papers100M-bin/raw/data.npz'\n",
    "TRAINING_FILE = '/mnt/nvme2/data/papers100M/raw/papers100M-bin/split/time/train.csv'\n",
    "\n",
    "# write destination\n",
    "STREAMING_EDGE_FILE = '/mnt/nvme2/data/papers100M/preprocessed2/streaming_edges.bin'\n",
    "CHUNK_INFO_FILE = '/mnt/nvme2/data/papers100M/preprocessed2/chunks.txt'\n",
    "RANDOM_READ_EDGE_FILE = '/mnt/nvme2/data/papers100M/preprocessed2/random_read_edges.bin'\n",
    "OFFSETS_FILE = '/mnt/nvme2/data/papers100M/preprocessed2/offsets.bin'\n",
    "TRAINING_BIN_FILE = '/mnt/nvme2/data/papers100M/preprocessed2/train.bin'\n",
    "\n",
    "SEQUENTIAL_READ_EDGE_FILE = '/mnt/nvme2/data/papers100M/preprocessed2/sequential_read_edges.bin'\n",
    "SEQUENTIAL_READ_CHUNK_INFO_FILE = '/mnt/nvme2/data/papers100M/preprocessed2/sequential_read_chunks.txt'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import struct\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load from raw files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOAD_FROM_RAW = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_degrees(edge_index, n_nodes):\n",
    "    degrees = np.zeros(n_nodes, dtype=np.uint32)\n",
    "    pbar = tqdm(total=edge_index[1].shape[0], desc='Computing degrees')\n",
    "    left = 0\n",
    "    pre_node = edge_index[1][0]\n",
    "    for i, node in enumerate(edge_index[1]):\n",
    "        if i % 10000000 == 0:\n",
    "            pbar.update(10000000)\n",
    "        \n",
    "        if node != pre_node:\n",
    "            degrees[pre_node] = i - left\n",
    "            left = i\n",
    "            pre_node = node\n",
    "\n",
    "    degrees[edge_index[1][-1]] = edge_index[1].shape[0] - left\n",
    "    pbar.close()\n",
    "    return degrees\n",
    "\n",
    "def sort_the_edges(edges, degrees):\n",
    "    pbar = tqdm(total=len(degrees), desc=\"Sort the neighbors of each node\")\n",
    "    pos = 0\n",
    "    for i, degree in enumerate(degrees):\n",
    "        if i % 100000 == 0:\n",
    "            pbar.update(100000)\n",
    "        \n",
    "        # assert degree >= 0, f\"Node {i} has negative degree\"\n",
    "        # assert is_assending(edges[pos:pos+degree]), f\"Edges of node {i} is not in ascending order, edges={edges[pos:pos+degree]}\"\n",
    "        edges[pos:pos+degree] = np.sort(edges[pos:pos+degree])\n",
    "\n",
    "        pos += degree\n",
    "    pbar.close()\n",
    "    return edges\n",
    "\n",
    "def compute_offsets(degrees):\n",
    "    offsets = np.zeros(len(degrees), dtype=np.uint32)\n",
    "    offsets[0] = 0\n",
    "    pbar = tqdm(total=len(degrees), desc=\"Compute offsets\")\n",
    "    for i in range(len(degrees) - 1):\n",
    "        offsets[i+1] = offsets[i] + degrees[i]\n",
    "        if i % 1000000 == 0:\n",
    "            pbar.update(1000000)\n",
    "    pbar.close()\n",
    "    return offsets\n",
    "\n",
    "def load_training_file(file_path):\n",
    "    df = pd.read_csv(file_path, header=None, names=['id'])\n",
    "    return np.array(df['id'].values, dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_nodes 111059956 edge_index.shape (2, 1615685872)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d135d2422914856a5ee4b9d592d893b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Computing degrees:   0%|          | 0/1615685872 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23accd4595e94e1e8f9ae489837ebb47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute offsets:   0%|          | 0/111059956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if LOAD_FROM_RAW:\n",
    "    data = np.load(EDGE_FILE)\n",
    "    n_nodes = data['num_nodes_list'][0]\n",
    "    edge_index = data['edge_index']\n",
    "    print('n_nodes', n_nodes, 'edge_index.shape', edge_index.shape)\n",
    "    degrees = compute_degrees(edge_index, n_nodes)\n",
    "    edges = np.array(edge_index[0], dtype=np.uint32)\n",
    "    # edges = sort_the_edges(edges, degrees)\n",
    "    offsets = compute_offsets(degrees)\n",
    "    train = load_training_file(TRAINING_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_assending(arr):\n",
    "    return np.all(np.diff(arr) >= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verification of raw data that should satistfy some assumtion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "VERIFY_RAW = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if VERIFY_RAW:\n",
    "    pre_node = -1\n",
    "    # pbar = tqdm(total=edge_index[1].shape[0],  desc=\"Verifying destination nodes are in ascending order\")\n",
    "    # for i, node in enumerate(edge_index[1]):\n",
    "    #     if i % 10000000 == 0:\n",
    "    #         pbar.update(10000000)\n",
    "    #     if node != pre_node:\n",
    "    #         assert node > pre_node, f\"Node ID {node} is not in ascending order\"\n",
    "    #         pre_node = node\n",
    "    print(is_assending(edge_index[1]))\n",
    "    # pbar.close()\n",
    "\n",
    "    pbar = tqdm(total=len(degrees), desc=\"Verifying source nodes are in ascending order\")\n",
    "    pos = 0\n",
    "    for i, degree in enumerate(degrees):\n",
    "        if i % 100000 == 0:\n",
    "            pbar.update(100000)\n",
    "        \n",
    "        assert degree >= 0, f\"Node {i} has negative degree\"\n",
    "        assert is_assending(edges[pos:pos+degree]), f\"Edges of node {i} is not in ascending order, edges={edges[pos:pos+degree]}\"\n",
    "\n",
    "        pos += degree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write the preprocessed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENABLE_WRITE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analysis_chunks(degrees, chunk_size=CHUNK_SIZE_BYTE):\n",
    "    max_ints = chunk_size // 4\n",
    "\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    current_chunk_size = 3 # total, start_id, first offsets\n",
    "    pbar = tqdm(total=len(degrees), desc=\"Analyzing chunks\")\n",
    "    for i in range(len(degrees)):\n",
    "        if i % 100000 == 0:\n",
    "            pbar.update(100000)\n",
    "        if current_chunk_size + degrees[i] + 1 > max_ints:\n",
    "            chunks.append((start, i-1))\n",
    "            start = i\n",
    "            current_chunk_size = 3\n",
    "        current_chunk_size += degrees[i] + 1\n",
    "    chunks.append((start, len(degrees)-1))\n",
    "    pbar.close()\n",
    "    return chunks\n",
    "\n",
    "def print_chunk_details(chunks, degrees):\n",
    "    print(\"Chunk Details\")\n",
    "    print(\"=\"*12*5)\n",
    "    print(f\"{'start':>12}{'end':>12}{'n_nodes':>12}{'size(bytes)':>12}{'fill up':>12}\")\n",
    "    for chunk in chunks:\n",
    "        start = chunk[0]\n",
    "        end = chunk[1]\n",
    "        n_nodes = sum(degrees[start:end+1])\n",
    "        size = (1 + 1 + (end - start + 2) + n_nodes ) * 4\n",
    "        print(f\"{start:12d}{end:12d}{n_nodes:12d}{size:12d}{size/CHUNK_SIZE_BYTE*100:9.2f}%\")\n",
    "\n",
    "    print(\"len(chunks)\", len(chunks))\n",
    "    print(\"chunks\", chunks)\n",
    "\n",
    "\n",
    "def create_padded_file(filepath, chunks, degrees, edges):\n",
    "    pbar = tqdm(total=len(chunks), desc=\"Creating padded file\")\n",
    "    with open(filepath, \"wb\") as file:\n",
    "        edges_cnt = 0\n",
    "        for t, chunk in enumerate(chunks):\n",
    "            # write buffer\n",
    "            byte_data_list = []\n",
    "            # total number of nodes, little endian, unsigned int\n",
    "            total_nodes = chunk[1] - chunk[0] + 1\n",
    "            byte_data_list.append(struct.pack(\"<I\", total_nodes))\n",
    "            # first node id, little endian, unsigned int\n",
    "            byte_data_list.append(struct.pack(\"<I\", chunk[0]))\n",
    "            # offsets, little endian, unsigned int\n",
    "            cnt = total_nodes + 2 + 1\n",
    "            byte_data_list.append(struct.pack(\"<I\", cnt))\n",
    "            for i in range(chunk[0], chunk[1]+1):\n",
    "                cnt += degrees[i]\n",
    "                byte_data_list.append(struct.pack(\"<I\", cnt))\n",
    "            # edges, little endian, unsigned int\n",
    "            for i in range(chunk[0], chunk[1]+1):\n",
    "                for j in range(edges_cnt, edges_cnt + degrees[i]):\n",
    "                    byte_data_list.append(struct.pack(\"<I\", edges[j]))\n",
    "                edges_cnt += degrees[i]\n",
    "            # padding zeros, unsigned int\n",
    "            # For the last chunk, padding to make the chunk size a multiple of 512\n",
    "            byte_data = b''.join(byte_data_list)\n",
    "            file.write(byte_data)\n",
    "            if t != len(chunks) - 1:\n",
    "                bytes_to_add = CHUNK_SIZE_BYTE - len(byte_data)\n",
    "            else:\n",
    "                bytes_to_add = 512 - len(byte_data) % 512\n",
    "            file.write(b'\\x00' * bytes_to_add)\n",
    "            pbar.update(1)\n",
    "            print(t, len(byte_data), bytes_to_add, len(byte_data) + bytes_to_add)\n",
    "            \n",
    "    pbar.close()\n",
    "\n",
    "def create_chunk_file(filepath, chunks):\n",
    "    with open(filepath, \"w\") as fh:\n",
    "        # fh.write(f\"{len(chunks)}\\n\")\n",
    "        for chunk in chunks:\n",
    "            # fh.write(f\"{chunk[0]} {chunk[1]}\\n\")\n",
    "            fh.write(f\"{chunk[1]}\\n\")\n",
    "\n",
    "def create_training_file(filepath, train):\n",
    "    train.tofile(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_seq_padded_file(filepath, chunks, degrees, edges):\n",
    "    pbar = tqdm(total=len(chunks), desc=\"Creating padded file\")\n",
    "    with open(filepath, \"wb\") as file:\n",
    "        edges_cnt = 0\n",
    "        for t, chunk in enumerate(chunks):\n",
    "            # write buffer\n",
    "            byte_data_list = []\n",
    "            # total number of nodes, little endian, unsigned int\n",
    "            total_nodes = chunk[1] - chunk[0] + 1\n",
    "            byte_data_list.append(struct.pack(\"<I\", total_nodes))\n",
    "            # first node id, little endian, unsigned int\n",
    "            byte_data_list.append(struct.pack(\"<I\", chunk[0]))\n",
    "            # offsets, little endian, unsigned int\n",
    "            for i in range(chunk[0], chunk[1]+1):\n",
    "                byte_data_list.append(struct.pack(\"<I\", degrees[i]))\n",
    "                for j in range(edges_cnt, edges_cnt + degrees[i]):\n",
    "                    byte_data_list.append(struct.pack(\"<I\", edges[j]))\n",
    "                edges_cnt += degrees[i]\n",
    "            \n",
    "            byte_data = b''.join(byte_data_list)\n",
    "            file.write(byte_data)\n",
    "            if t != len(chunks) - 1:\n",
    "                bytes_to_add = CHUNK_SIZE_BYTE - len(byte_data)\n",
    "            else:\n",
    "                bytes_to_add = 512 - len(byte_data) % 512\n",
    "            file.write(b'\\x00' * bytes_to_add)\n",
    "            pbar.update(1)\n",
    "            print(t, len(byte_data), bytes_to_add, len(byte_data) + bytes_to_add)\n",
    "            \n",
    "    pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e39d016ef1524b41aae4f8159ce3a61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing chunks:   0%|          | 0/111059956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Details\n",
      "============================================================\n",
      "       start         end     n_nodes size(bytes)     fill up\n",
      "           0    10521746   123695935   536870740   100.00%\n",
      "    10521747    16564523   128167549   536841316    99.99%\n",
      "    16564524    21486461   129295773   536870856   100.00%\n",
      "    21486462    26399870   129304299   536870844   100.00%\n",
      "    26399871    31310495   129306910   536870152   100.00%\n",
      "    31310496    36218482   129309715   536870820   100.00%\n",
      "    36218483    41128311   129307864   536870784   100.00%\n",
      "    41128312    45803992   129540167   536863404   100.00%\n",
      "    45803993    49503593   130518124   536870912   100.00%\n",
      "    49503594    53191139   130530115   536870656   100.00%\n",
      "    53191140    63352413   124056435   536870848   100.00%\n",
      "    63352414    80917735   116652065   536869560   100.00%\n",
      "    80917736   111059955    86000921   464572576    86.53%\n",
      "len(chunks) 13\n",
      "chunks [(0, 10521746), (10521747, 16564523), (16564524, 21486461), (21486462, 26399870), (26399871, 31310495), (31310496, 36218482), (36218483, 41128311), (41128312, 45803992), (45803993, 49503593), (49503594, 53191139), (53191140, 63352413), (63352414, 80917735), (80917736, 111059955)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ae71a45262d4276bdf08085b28bf4a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating padded file:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 536870736 176 536870912\n",
      "1 536841312 29600 536870912\n",
      "2 536870852 60 536870912\n",
      "3 536870840 72 536870912\n",
      "4 536870148 764 536870912\n",
      "5 536870816 96 536870912\n",
      "6 536870780 132 536870912\n",
      "7 536863400 7512 536870912\n",
      "8 536870908 4 536870912\n",
      "9 536870652 260 536870912\n",
      "10 536870844 68 536870912\n",
      "11 536869556 1356 536870912\n",
      "12 464572572 356 464572928\n"
     ]
    }
   ],
   "source": [
    "# seq kernel\n",
    "\n",
    "if ENABLE_WRITE:\n",
    "    chunks = analysis_chunks(degrees)\n",
    "    print_chunk_details(chunks, degrees)\n",
    "    create_seq_padded_file(SEQUENTIAL_READ_EDGE_FILE, chunks, degrees, edges)\n",
    "    create_chunk_file(SEQUENTIAL_READ_CHUNK_INFO_FILE, chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "886cfd66723c4829964bd2d67f462c47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Analyzing chunks:   0%|          | 0/111059956 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk Details\n",
      "============================================================\n",
      "       start         end     n_nodes size(bytes)     fill up\n",
      "           0    10521746   123695935   536870740   100.00%\n",
      "    10521747    16564523   128167549   536841316    99.99%\n",
      "    16564524    21486461   129295773   536870856   100.00%\n",
      "    21486462    26399870   129304299   536870844   100.00%\n",
      "    26399871    31310495   129306910   536870152   100.00%\n",
      "    31310496    36218482   129309715   536870820   100.00%\n",
      "    36218483    41128311   129307864   536870784   100.00%\n",
      "    41128312    45803992   129540167   536863404   100.00%\n",
      "    45803993    49503593   130518124   536870912   100.00%\n",
      "    49503594    53191139   130530115   536870656   100.00%\n",
      "    53191140    63352413   124056435   536870848   100.00%\n",
      "    63352414    80917735   116652065   536869560   100.00%\n",
      "    80917736   111059955    86000921   464572576    86.53%\n",
      "len(chunks) 13\n",
      "chunks [(0, 10521746), (10521747, 16564523), (16564524, 21486461), (21486462, 26399870), (26399871, 31310495), (31310496, 36218482), (36218483, 41128311), (41128312, 45803992), (45803993, 49503593), (49503594, 53191139), (53191140, 63352413), (63352414, 80917735), (80917736, 111059955)]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f64d397bed4908952c92a93c8daec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating padded file:   0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 536870740 172 536870912\n",
      "1 536841316 29596 536870912\n",
      "2 536870856 56 536870912\n",
      "3 536870844 68 536870912\n",
      "4 536870152 760 536870912\n",
      "5 536870820 92 536870912\n",
      "6 536870784 128 536870912\n",
      "7 536863404 7508 536870912\n",
      "8 536870912 0 536870912\n",
      "9 536870656 256 536870912\n",
      "10 536870848 64 536870912\n",
      "11 536869560 1352 536870912\n",
      "12 464572576 352 464572928\n"
     ]
    }
   ],
   "source": [
    "if ENABLE_WRITE:\n",
    "    chunks = analysis_chunks(degrees)\n",
    "    print_chunk_details(chunks, degrees)\n",
    "    create_padded_file(STREAMING_EDGE_FILE, chunks, degrees, edges)\n",
    "    create_chunk_file(CHUNK_INFO_FILE, chunks)\n",
    "    create_training_file(TRAINING_BIN_FILE, train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREATE_RANDOM_READ_SAMPLER_FILE = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_file_to_512_multiple(edges, output_file_path):\n",
    "    # write the whole edges as binary file\n",
    "    print(edges.dtype.byteorder)\n",
    "    assert edges.dtype.byteorder != '>', \"Numpy array shoule not be big endian\"\n",
    "    edges.tofile(output_file_path)\n",
    "    padding_needed = 512 - (len(edges) * 4) % 512\n",
    "\n",
    "    if padding_needed > 0:\n",
    "         with open(output_file_path, 'ab') as file:\n",
    "            file.write(b'\\x00' * padding_needed)\n",
    "\n",
    "    print(f\"File {output_file_path} has been padded with {padding_needed} bytes.\")\n",
    "    \n",
    "\n",
    "def create_offsets_file(offsets, output_file_path):\n",
    "    offsets.tofile(output_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=\n",
      "File /mnt/nvme2/data/papers100M/preprocessed2/random_read_edges.bin has been padded with 64 bytes.\n"
     ]
    }
   ],
   "source": [
    "if CREATE_RANDOM_READ_SAMPLER_FILE:\n",
    "    pad_file_to_512_multiple(edges, RANDOM_READ_EDGE_FILE)\n",
    "    create_offsets_file(offsets, OFFSETS_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing for random read sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File /mnt/nvme2/data/yahoo/preprocessed/streaming_edges.bin has been padded with 1024 bytes.\n"
     ]
    }
   ],
   "source": [
    "def pad_file_to_512_multiple(file_path):\n",
    "    # Open the file in binary mode\n",
    "    with open(file_path, 'ab') as file:\n",
    "        # Determine the current file size\n",
    "        file.seek(0, 2)  # Move to the end of the file\n",
    "        file_size = file.tell()\n",
    "        \n",
    "        # Calculate the padding needed\n",
    "        padding_needed = (512 - (file_size % 512)) + 512\n",
    "        \n",
    "        # Append zeros to pad the file to a multiple of 512 bytes\n",
    "        if padding_needed > 0:\n",
    "            file.write(b'\\x00' * padding_needed)\n",
    "            \n",
    "        print(f\"File {file_path} has been padded with {padding_needed} bytes.\")\n",
    "\n",
    "# Example usage\n",
    "file_path = '/mnt/nvme2/data/yahoo/preprocessed/streaming_edges.bin'\n",
    "pad_file_to_512_multiple(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# streaming_edges = np.fromfile( '/mnt/nvme2/data/papers100M/preprocessed/edges_padded.bin', dtype=np.uint32)\n",
    "streaming_edges = np.fromfile( STREAMING_EDGE_FILE, dtype=np.uint32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1077441427 134217728\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(streaming_edges)):\n",
    "    if streaming_edges[i] == 134217728:\n",
    "        print(i, streaming_edges[i])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5760cf421a5f4edeb14d052290934c33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/13 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 10521747 0 134217728\n",
      "1 6042777 10521747 134217728\n",
      "2 4921938 16564524 134217728\n",
      "3 4913409 21486462 134217728\n",
      "4 4910625 26399871 134217728\n",
      "5 4907987 31310496 134217728\n",
      "6 4909829 36218483 134217728\n",
      "7 4675681 41128312 134217728\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m start_node \u001b[38;5;241m=\u001b[39m streaming_edges[start_pos \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      6\u001b[0m cur_offsets \u001b[38;5;241m=\u001b[39m streaming_edges[start_pos \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m: start_pos \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m cur_total \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28mprint\u001b[39m(t, cur_total, start_node, \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mstreaming_edges\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_pos\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcur_total\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m is_assending \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mall(cur_offsets[\u001b[38;5;241m1\u001b[39m:] \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m cur_offsets[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m is_assending, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not in ascending order\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "pbar = tqdm(total=13)\n",
    "for t in range(13):\n",
    "    start_pos = t * CHUNK_SIZE_BYTE // 4\n",
    "    cur_total = streaming_edges[start_pos + 0]\n",
    "    start_node = streaming_edges[start_pos + 1]\n",
    "    cur_offsets = streaming_edges[start_pos + 2: start_pos + 2 + cur_total + 1]\n",
    "    print(t, cur_total, start_node, max(streaming_edges[start_pos + 2 + cur_total + 1:]))\n",
    "    is_assending = np.all(cur_offsets[1:] >= cur_offsets[:-1])\n",
    "    assert is_assending, f\"Chunk {t} is not in ascending order\"\n",
    "    pbar.update(1)\n",
    "pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "111059955"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(edges)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
